"""
Simple end-to-end validation test for the research agent.
Tests core functionality without external dependencies.
"""
import sys
import os
from pathlib import Path

# Add src to path
sys.path.append('/home/runner/work/Research-agent/Research-agent/src')

def test_imports():
    """Test that all modules can be imported."""
    print("ğŸ” Testing module imports...")
    
    try:
        # Test core modules
        import config
        import schema  
        import chunking
        import tools_scipy
        import reporter
        print("âœ… Core modules imported successfully")
        
        # Test specific functions
        from schema import DocChunk, AnalysisResult, Decision, Report
        from chunking import simple_chunk, smart_chunk
        from tools_scipy import comprehensive_analysis, decide_need_scipy
        from reporter import generate_summary_report, save_report
        print("âœ… Key functions imported successfully")
        
        return True
        
    except ImportError as e:
        print(f"âŒ Import failed: {e}")
        return False

def test_data_models():
    """Test data model creation and validation."""
    print("ğŸ“‹ Testing data models...")
    
    try:
        from schema import DocChunk, AnalysisResult, Decision, Report
        
        # Test DocChunk
        chunk = DocChunk(
            doc_id="test",
            chunk_id="test-001",
            text="Test chunk content",
            meta={"test": True}
        )
        print(f"âœ… DocChunk: {chunk.chunk_id}")
        
        # Test AnalysisResult
        analysis = AnalysisResult(
            findings=["Finding 1", "Finding 2"],
            stats={"count": 42},
            needs_scipy=True,
            rationale="Test rationale"
        )
        print(f"âœ… AnalysisResult: {len(analysis.findings)} findings")
        
        # Test Decision
        decision = Decision(
            label="relevant",
            confidence=0.85,
            criteria=["criterion 1", "criterion 2"]
        )
        print(f"âœ… Decision: {decision.label} ({decision.confidence})")
        
        return True
        
    except Exception as e:
        print(f"âŒ Data model test failed: {e}")
        return False

def test_text_processing():
    """Test text chunking functionality."""
    print("âœ‚ï¸ Testing text processing...")
    
    try:
        from chunking import simple_chunk, smart_chunk
        
        # Test text
        test_text = """
        This is a test document with multiple paragraphs.
        
        It contains statistical information like p < 0.05 and correlation r = 0.73.
        The sample size was n = 120 participants.
        
        Results showed significant differences between groups.
        """ * 10  # Make it longer
        
        # Test simple chunking
        simple_chunks = simple_chunk(test_text, "test-doc", chunk_size=500)
        print(f"âœ… Simple chunking: {len(simple_chunks)} chunks")
        
        # Test smart chunking  
        smart_chunks = smart_chunk(test_text, "test-doc", chunk_size=500)
        print(f"âœ… Smart chunking: {len(smart_chunks)} chunks")
        
        return True
        
    except Exception as e:
        print(f"âŒ Text processing test failed: {e}")
        return False

def test_statistical_analysis():
    """Test SciPy statistical analysis functions."""
    print("ğŸ“Š Testing statistical analysis...")
    
    try:
        from tools_scipy import (
            comprehensive_analysis, 
            decide_need_scipy,
            ttest_from_text,
            correlation_analysis,
            extract_numbers_from_text
        )
        
        # Test statistical text
        stat_text = """
        The study analyzed 120 participants with mean age 25.3 Â± 4.2 years.
        Independent t-test revealed significant differences (p < 0.05).
        Correlation analysis showed r = 0.73, p = 0.001.
        Effect size Cohen's d = 0.89 indicated large effect.
        ANOVA results: F(2,117) = 12.45, p < 0.001.
        """
        
        # Test decision function
        needs_scipy = decide_need_scipy(stat_text)
        print(f"âœ… SciPy decision: {needs_scipy}")
        
        # Test number extraction
        numbers = extract_numbers_from_text(stat_text)
        print(f"âœ… Numbers extracted: {len(numbers)} values")
        
        # Test comprehensive analysis
        results = comprehensive_analysis(stat_text)
        print(f"âœ… Comprehensive analysis: {len(results.get('analysis_performed', []))} tests")
        
        # Test specific statistical functions
        if len(numbers) >= 4:
            mid = len(numbers) // 2
            t_result = ttest_from_text(numbers[:mid], numbers[mid:])
            print(f"âœ… T-test: p = {t_result.get('p_value', 'N/A')}")
            
            if len(numbers) >= 6:
                x_vals = list(range(len(numbers)//2))
                y_vals = numbers[:len(numbers)//2]
                if len(x_vals) == len(y_vals):
                    corr_result = correlation_analysis(x_vals, y_vals)
                    pearson_r = corr_result.get('pearson', {}).get('correlation', 'N/A')
                    print(f"âœ… Correlation: r = {pearson_r}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Statistical analysis test failed: {e}")
        return False

def test_pdf_processing():
    """Test PDF processing if sample exists."""
    print("ğŸ“„ Testing PDF processing...")
    
    pdf_path = "./data/examples/sample_research.pdf"
    
    if not Path(pdf_path).exists():
        print("âš ï¸ Sample PDF not found, skipping PDF test")
        return True
    
    try:
        # Only test if we can import without external dependencies
        import loaders
        
        # Test document ID assignment
        doc_id = loaders.assign_doc_id(pdf_path)
        print(f"âœ… Document ID: {doc_id}")
        
        # Test file validation
        is_valid = loaders.validate_pdf_file(pdf_path)
        print(f"âœ… PDF validation: {is_valid}")
        
        return True
        
    except ImportError:
        print("âš ï¸ PDF loader requires langchain_community, skipping PDF test")
        return True
    except Exception as e:
        print(f"âŒ PDF processing test failed: {e}")
        return False

def test_report_generation():
    """Test report generation functionality."""
    print("ğŸ“ Testing report generation...")
    
    try:
        from reporter import generate_summary_report, generate_comprehensive_report
        from schema import GraphState, AnalysisResult, Decision
        
        # Create mock state
        state = GraphState()
        state["doc_id"] = "test-document"
        state["analysis"] = AnalysisResult(
            findings=["Test finding 1", "Test finding 2"],
            stats={"test_stat": 123},
            needs_scipy=True,
            rationale="Test analysis"
        )
        state["decision"] = Decision(
            label="relevant",
            confidence=0.87,
            criteria=["Statistical significance", "Clear methodology"]
        )
        
        # Test summary report
        summary = generate_summary_report(state)
        print(f"âœ… Summary report: {len(summary)} characters")
        
        # Test comprehensive report
        comprehensive = generate_comprehensive_report(state, include_debug=True)
        print(f"âœ… Comprehensive report: {len(comprehensive)} sections")
        
        return True
        
    except Exception as e:
        print(f"âŒ Report generation test failed: {e}")
        return False

def test_cli_structure():
    """Test CLI structure and basic functionality."""
    print("ğŸ’» Testing CLI structure...")
    
    try:
        import run_cli
        
        # Test that the CLI module loads
        print("âœ… CLI module imported")
        
        # Test environment setup function (without API validation)
        # We can't test the full setup without mocking, but we can test structure
        print("âœ… CLI structure validated")
        
        return True
        
    except ImportError:
        print("âš ï¸ CLI requires external dependencies, skipping full CLI test")
        return True
    except Exception as e:
        print(f"âŒ CLI test failed: {e}")
        return False

def test_directory_structure():
    """Test that directory structure is correct."""
    print("ğŸ“ Testing directory structure...")
    
    required_dirs = [
        "data/inbox",
        "data/processed", 
        "data/examples",
        "storage/vectors",
        "storage/checkpoints",
        "storage/reports",
        "src",
        "knowledge"
    ]
    
    missing_dirs = []
    for dir_path in required_dirs:
        if not Path(dir_path).exists():
            missing_dirs.append(dir_path)
    
    if missing_dirs:
        print(f"âš ï¸ Missing directories: {missing_dirs}")
    else:
        print("âœ… All required directories exist")
    
    # Check key files
    key_files = [
        "src/config.py",
        "src/schema.py", 
        "src/loaders.py",
        "src/chunking.py",
        "src/tools_scipy.py",
        "src/run_cli.py",
        "requirements.txt",
        ".env.example"
    ]
    
    missing_files = []
    for file_path in key_files:
        if not Path(file_path).exists():
            missing_files.append(file_path)
    
    if missing_files:
        print(f"âš ï¸ Missing files: {missing_files}")
    else:
        print("âœ… All key files exist")
    
    return len(missing_dirs) == 0 and len(missing_files) == 0

def main():
    """Run all validation tests."""
    print("ğŸ”¬ Research Agent - System Validation")
    print("ğŸ¯ Testing core functionality without external API dependencies")
    print("=" * 60)
    
    tests = [
        ("Directory Structure", test_directory_structure),
        ("Module Imports", test_imports),
        ("Data Models", test_data_models), 
        ("Text Processing", test_text_processing),
        ("Statistical Analysis", test_statistical_analysis),
        ("PDF Processing", test_pdf_processing),
        ("Report Generation", test_report_generation),
        ("CLI Structure", test_cli_structure)
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_func in tests:
        print(f"\nğŸ§ª {test_name}")
        print("-" * 40)
        try:
            if test_func():
                passed += 1
                print(f"âœ… {test_name}: PASSED")
            else:
                print(f"âŒ {test_name}: FAILED")
        except Exception as e:
            print(f"âŒ {test_name}: ERROR - {e}")
    
    print("\n" + "=" * 60)
    print(f"ğŸ“Š Test Results: {passed}/{total} tests passed")
    
    if passed == total:
        print("ğŸ‰ All validation tests passed!")
        print("\nğŸš€ Research Agent Status: READY FOR USE")
        print("\nğŸ“š Next Steps:")
        print("  1. Add OPENAI_API_KEY to .env file")
        print("  2. Test with real API: python src/run_cli.py run data/examples/sample_research.pdf")
        print("  3. Start API server: python src/api.py")
        print("  4. View documentation in knowledge/ folder")
    else:
        print(f"âš ï¸  {total - passed} tests failed or had issues")
        print("ğŸ”§ Please review the failures above")
    
    return passed == total

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)