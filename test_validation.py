"""
Simple end-to-end validation test for the research agent.
Tests core functionality without external dependencies.
"""
import sys
import os
from pathlib import Path

# Add src to path
sys.path.append('/home/runner/work/Research-agent/Research-agent/src')

def test_imports():
    """Test that all modules can be imported."""
    print("üîç Testing module imports...")
    
    try:
        # Test core modules
        import config
        import schema  
        import chunking
        import tools_scipy
        import reporter
        print("‚úÖ Core modules imported successfully")
        
        # Test specific functions
        from schema import DocChunk, AnalysisResult, Decision, Report
        from chunking import simple_chunk, smart_chunk
        from tools_scipy import comprehensive_analysis, decide_need_scipy
        from reporter import generate_summary_report, save_report
        print("‚úÖ Key functions imported successfully")
        
        return True
        
    except ImportError as e:
        print(f"‚ùå Import failed: {e}")
        return False

def test_data_models():
    """Test data model creation and validation."""
    print("üìã Testing data models...")
    
    try:
        from schema import DocChunk, AnalysisResult, Decision, Report
        
        # Test DocChunk
        chunk = DocChunk(
            doc_id="test",
            chunk_id="test-001",
            text="Test chunk content",
            meta={"test": True}
        )
        print(f"‚úÖ DocChunk: {chunk.chunk_id}")
        
        # Test AnalysisResult
        analysis = AnalysisResult(
            findings=["Finding 1", "Finding 2"],
            stats={"count": 42},
            needs_scipy=True,
            rationale="Test rationale"
        )
        print(f"‚úÖ AnalysisResult: {len(analysis.findings)} findings")
        
        # Test Decision
        decision = Decision(
            label="relevant",
            confidence=0.85,
            criteria=["criterion 1", "criterion 2"]
        )
        print(f"‚úÖ Decision: {decision.label} ({decision.confidence})")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Data model test failed: {e}")
        return False

def test_text_processing():
    """Test text chunking functionality."""
    print("‚úÇÔ∏è Testing text processing...")
    
    try:
        from chunking import simple_chunk, smart_chunk
        
        # Test text
        test_text = """
        This is a test document with multiple paragraphs.
        
        It contains statistical information like p < 0.05 and correlation r = 0.73.
        The sample size was n = 120 participants.
        
        Results showed significant differences between groups.
        """ * 10  # Make it longer
        
        # Test simple chunking
        simple_chunks = simple_chunk(test_text, "test-doc", chunk_size=500)
        print(f"‚úÖ Simple chunking: {len(simple_chunks)} chunks")
        
        # Test smart chunking  
        smart_chunks = smart_chunk(test_text, "test-doc", chunk_size=500)
        print(f"‚úÖ Smart chunking: {len(smart_chunks)} chunks")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Text processing test failed: {e}")
        return False

def test_statistical_analysis():
    """Test SciPy statistical analysis functions."""
    print("üìä Testing statistical analysis...")
    
    try:
        from tools_scipy import (
            comprehensive_analysis, 
            decide_need_scipy,
            ttest_from_text,
            correlation_analysis,
            extract_numbers_from_text
        )
        
        # Test statistical text
        stat_text = """
        The study analyzed 120 participants with mean age 25.3 ¬± 4.2 years.
        Independent t-test revealed significant differences (p < 0.05).
        Correlation analysis showed r = 0.73, p = 0.001.
        Effect size Cohen's d = 0.89 indicated large effect.
        ANOVA results: F(2,117) = 12.45, p < 0.001.
        """
        
        # Test decision function
        needs_scipy = decide_need_scipy(stat_text)
        print(f"‚úÖ SciPy decision: {needs_scipy}")
        
        # Test number extraction
        numbers = extract_numbers_from_text(stat_text)
        print(f"‚úÖ Numbers extracted: {len(numbers)} values")
        
        # Test comprehensive analysis
        results = comprehensive_analysis(stat_text)
        print(f"‚úÖ Comprehensive analysis: {len(results.get('analysis_performed', []))} tests")
        
        # Test specific statistical functions
        if len(numbers) >= 4:
            mid = len(numbers) // 2
            t_result = ttest_from_text(numbers[:mid], numbers[mid:])
            print(f"‚úÖ T-test: p = {t_result.get('p_value', 'N/A')}")
            
            if len(numbers) >= 6:
                x_vals = list(range(len(numbers)//2))
                y_vals = numbers[:len(numbers)//2]
                if len(x_vals) == len(y_vals):
                    corr_result = correlation_analysis(x_vals, y_vals)
                    pearson_r = corr_result.get('pearson', {}).get('correlation', 'N/A')
                    print(f"‚úÖ Correlation: r = {pearson_r}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Statistical analysis test failed: {e}")
        return False

def test_pdf_processing():
    """Test PDF processing if sample exists."""
    print("üìÑ Testing PDF processing...")
    
    pdf_path = "./data/examples/sample_research.pdf"
    
    if not Path(pdf_path).exists():
        print("‚ö†Ô∏è Sample PDF not found, skipping PDF test")
        return True
    
    try:
        # Only test if we can import without external dependencies
        import loaders
        
        # Test document ID assignment
        doc_id = loaders.assign_doc_id(pdf_path)
        print(f"‚úÖ Document ID: {doc_id}")
        
        # Test file validation
        is_valid = loaders.validate_pdf_file(pdf_path)
        print(f"‚úÖ PDF validation: {is_valid}")
        
        return True
        
    except ImportError:
        print("‚ö†Ô∏è PDF loader requires langchain_community, skipping PDF test")
        return True
    except Exception as e:
        print(f"‚ùå PDF processing test failed: {e}")
        return False

def test_report_generation():
    """Test report generation functionality."""
    print("üìù Testing report generation...")
    
    try:
        from reporter import generate_summary_report, generate_comprehensive_report
        from schema import GraphState, AnalysisResult, Decision
        
        # Create mock state
        state = GraphState()
        state["doc_id"] = "test-document"
        state["analysis"] = AnalysisResult(
            findings=["Test finding 1", "Test finding 2"],
            stats={"test_stat": 123},
            needs_scipy=True,
            rationale="Test analysis"
        )
        state["decision"] = Decision(
            label="relevant",
            confidence=0.87,
            criteria=["Statistical significance", "Clear methodology"]
        )
        
        # Test summary report
        summary = generate_summary_report(state)
        print(f"‚úÖ Summary report: {len(summary)} characters")
        
        # Test comprehensive report
        comprehensive = generate_comprehensive_report(state, include_debug=True)
        print(f"‚úÖ Comprehensive report: {len(comprehensive)} sections")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Report generation test failed: {e}")
        return False

def test_cli_structure():
    """Test CLI structure and basic functionality."""
    print("üíª Testing CLI structure...")
    
    try:
        import run_cli
        
        # Test that the CLI module loads
        print("‚úÖ CLI module imported")
        
        # Test environment setup function (without API validation)
        # We can't test the full setup without mocking, but we can test structure
        print("‚úÖ CLI structure validated")
        
        return True
        
    except ImportError:
        print("‚ö†Ô∏è CLI requires external dependencies, skipping full CLI test")
        return True
    except Exception as e:
        print(f"‚ùå CLI test failed: {e}")
        return False

def test_directory_structure():
    """Test that directory structure is correct."""
    print("üìÅ Testing directory structure...")
    
    required_dirs = [
        "data/inbox",
        "data/processed", 
        "data/examples",
        "storage/vectors",
        "storage/checkpoints",
        "storage/reports",
        "src",
        "knowledge"
    ]
    
    missing_dirs = []
    for dir_path in required_dirs:
        if not Path(dir_path).exists():
            missing_dirs.append(dir_path)
    
    if missing_dirs:
        print(f"‚ö†Ô∏è Missing directories: {missing_dirs}")
    else:
        print("‚úÖ All required directories exist")
    
    # Check key files
    key_files = [
        "src/config.py",
        "src/schema.py", 
        "src/loaders.py",
        "src/chunking.py",
        "src/tools_scipy.py",
        "src/run_cli.py",
        "requirements.txt",
        ".env.example"
    ]
    
    missing_files = []
    for file_path in key_files:
        if not Path(file_path).exists():
            missing_files.append(file_path)
    
    if missing_files:
        print(f"‚ö†Ô∏è Missing files: {missing_files}")
    else:
        print("‚úÖ All key files exist")
    
    return len(missing_dirs) == 0 and len(missing_files) == 0

def main():
    """Run all validation tests."""
    print("üî¨ Research Agent - System Validation")
    print("üéØ Testing core functionality without external API dependencies")
    print("=" * 60)
    
    tests = [
        ("Directory Structure", test_directory_structure),
        ("Module Imports", test_imports),
        ("Data Models", test_data_models), 
        ("Text Processing", test_text_processing),
        ("Statistical Analysis", test_statistical_analysis),
        ("PDF Processing", test_pdf_processing),
        ("Report Generation", test_report_generation),
        ("CLI Structure", test_cli_structure)
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_func in tests:
        print(f"\nüß™ {test_name}")
        print("-" * 40)
        try:
            if test_func():
                passed += 1
                print(f"‚úÖ {test_name}: PASSED")
            else:
                print(f"‚ùå {test_name}: FAILED")
        except Exception as e:
            print(f"‚ùå {test_name}: ERROR - {e}")
    
    print("\n" + "=" * 60)
    print(f"üìä Test Results: {passed}/{total} tests passed")
    
    if passed == total:
        print("üéâ All validation tests passed!")
        print("\nüöÄ Research Agent Status: READY FOR USE")
        print("\nüìö Next Steps:")
        print("  1. Add OPENAI_API_KEY to .env file")
        print("  2. Test with real API: python src/run_cli.py run data/examples/sample_research.pdf")
        print("  3. Start API server: python src/api.py")
        print("  4. View documentation in knowledge/ folder")
    else:
        print(f"‚ö†Ô∏è  {total - passed} tests failed or had issues")
        print("üîß Please review the failures above")
    
    return passed == total

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)